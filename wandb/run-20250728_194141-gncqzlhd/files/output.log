=== Stage 1: Zielgeschwindigkeit 0.1 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([ 0.0088,  0.0048, -0.1856, -0.0007, -0.0090,  0.0050, -0.0004, -0.0004,
         0.0005, -0.0005, -0.0005, -0.0005, -0.0005, -0.0005, -0.0029, -0.0029,
         0.0036, -0.0033, -0.0035, -0.0034, -0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-1.1980, -0.5001,  0.6319, -0.1854, -1.0273, -3.0135, -0.4585, -0.6896],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0042, device='cuda:0')
[WandB] Iter 0 | reward=-0.00 | loss=0.0015
[Plot] saved to logs/dodo-walking/metrics.png
[WandB] Iter 1 | reward=-0.00 | loss=0.0032
[WandB] Iter 2 | reward=-0.00 | loss=0.0007
[WandB] Iter 3 | reward=-0.00 | loss=0.0005
[WandB] Iter 4 | reward=-0.00 | loss=0.0006
[WandB] Iter 5 | reward=-0.00 | loss=0.0006
[WandB] Iter 6 | reward=-0.00 | loss=0.0442
[WandB] Iter 7 | reward=0.00 | loss=0.0033
[WandB] Iter 8 | reward=0.00 | loss=0.0008
[WandB] Iter 9 | reward=0.00 | loss=0.0007
[DEBUG] Observation (env 0): tensor([ 0.0002, -0.0204, -0.1883, -0.0010, -0.0073, -0.0209, -0.0004, -0.0004,
         0.0005,  0.0004,  0.0005,  0.0005,  0.0005,  0.0005, -0.0029, -0.0029,
         0.0031,  0.0030,  0.0033,  0.0033,  0.0036,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-0.3361, -1.7418,  0.5683,  0.7601,  1.0769,  1.4498,  1.2579,  0.3328],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0068, device='cuda:0')
[WandB] Iter 10 | reward=0.00 | loss=0.0006
[WandB] Iter 11 | reward=-0.00 | loss=0.0009
[WandB] Iter 12 | reward=-0.00 | loss=0.0007
[WandB] Iter 13 | reward=-0.00 | loss=0.0006
[WandB] Iter 14 | reward=-0.00 | loss=0.0006
[WandB] Iter 15 | reward=0.00 | loss=0.0006
[WandB] Iter 16 | reward=0.00 | loss=0.0005
[WandB] Iter 17 | reward=0.00 | loss=0.0006
[WandB] Iter 18 | reward=0.00 | loss=0.0006
[WandB] Iter 19 | reward=0.00 | loss=0.0007
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage1.pt
=== Stage 2: Zielgeschwindigkeit 0.3 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([ 0.0088, -0.0167, -0.1839, -0.0102, -0.0130, -0.0067,  0.0005, -0.0005,
         0.0005, -0.0005,  0.0005,  0.0005,  0.0005, -0.0005,  0.0034, -0.0034,
         0.0034, -0.0036,  0.0034,  0.0035,  0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.2004, -1.0148,  1.1632, -0.4399,  0.3643,  0.3116,  0.7691, -0.6771],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0051, device='cuda:0')
[WandB] Iter 20 | reward=-0.00 | loss=0.0030
[WandB] Iter 21 | reward=-0.00 | loss=0.0007
[WandB] Iter 22 | reward=-0.00 | loss=0.0012
[WandB] Iter 23 | reward=-0.00 | loss=0.0006
[WandB] Iter 24 | reward=0.00 | loss=0.0447
[WandB] Iter 25 | reward=0.00 | loss=0.0256
[WandB] Iter 26 | reward=0.00 | loss=0.0014
[WandB] Iter 27 | reward=0.00 | loss=0.0009
[WandB] Iter 28 | reward=0.00 | loss=0.0006
[WandB] Iter 29 | reward=0.00 | loss=0.0006
[DEBUG] Observation (env 0): tensor([ 2.4609e-04, -1.0142e-02, -1.9264e-01,  4.8409e-04, -3.9572e-03,
        -1.0611e-02,  4.3668e-04,  4.3855e-04,  5.0803e-04,  5.0373e-04,
        -5.3777e-04, -5.4126e-04,  5.3041e-04, -5.3085e-04,  2.9112e-03,
         2.9236e-03,  3.3869e-03,  3.3582e-03, -3.5851e-03, -3.6084e-03,
         3.5360e-03, -3.5390e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         3.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.0969,  1.1537, -0.0530,  0.2722,  0.1945, -1.6056,  0.3713, -0.4846],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0072, device='cuda:0')
[WandB] Iter 30 | reward=0.00 | loss=0.0006
[WandB] Iter 31 | reward=-0.00 | loss=0.0006
[WandB] Iter 32 | reward=-0.00 | loss=0.0007
[WandB] Iter 33 | reward=-0.00 | loss=0.0007
[WandB] Iter 34 | reward=0.00 | loss=0.0006
[WandB] Iter 35 | reward=0.00 | loss=0.0007
[WandB] Iter 36 | reward=0.00 | loss=0.0006
[WandB] Iter 37 | reward=0.00 | loss=0.0007
[WandB] Iter 38 | reward=0.00 | loss=0.0006
[WandB] Iter 39 | reward=0.00 | loss=0.0005
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage2.pt
=== Stage 3: Zielgeschwindigkeit 0.4 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([ 0.0120, -0.0037, -0.1818,  0.0006, -0.0164, -0.0039,  0.0004,  0.0004,
         0.0005, -0.0005,  0.0005, -0.0005,  0.0005, -0.0005,  0.0029,  0.0029,
         0.0035, -0.0034,  0.0035, -0.0034,  0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.7363,  0.8535,  0.5073,  0.0208,  0.3132, -1.9128,  0.8383, -2.1321],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0061, device='cuda:0')
[WandB] Iter 40 | reward=-0.00 | loss=0.0023
[WandB] Iter 41 | reward=-0.00 | loss=0.0212
[WandB] Iter 42 | reward=-0.00 | loss=0.0006
[WandB] Iter 43 | reward=0.00 | loss=0.0006
[WandB] Iter 44 | reward=0.00 | loss=0.0007
[WandB] Iter 45 | reward=0.00 | loss=0.0006
[WandB] Iter 46 | reward=0.00 | loss=0.0006
[WandB] Iter 47 | reward=0.00 | loss=0.0006
[WandB] Iter 48 | reward=0.00 | loss=0.0006
[WandB] Iter 49 | reward=0.00 | loss=0.0006
[DEBUG] Observation (env 0): tensor([ 0.0120, -0.0044, -0.1816,  0.0006, -0.0167, -0.0046,  0.0004,  0.0004,
         0.0005, -0.0005,  0.0005, -0.0005, -0.0005,  0.0005,  0.0029,  0.0029,
         0.0035, -0.0034,  0.0035, -0.0035, -0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.4570,  0.3210,  0.6305, -0.3306,  0.8877, -1.3786, -0.7789,  2.1985],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0077, device='cuda:0')
[WandB] Iter 50 | reward=0.00 | loss=0.0006
[WandB] Iter 51 | reward=-0.00 | loss=0.0016
[WandB] Iter 52 | reward=-0.00 | loss=0.0008
[WandB] Iter 53 | reward=0.00 | loss=0.0006
[WandB] Iter 54 | reward=0.00 | loss=0.0006
[WandB] Iter 55 | reward=0.00 | loss=0.0007
[WandB] Iter 56 | reward=0.00 | loss=0.0006
[WandB] Iter 57 | reward=0.00 | loss=0.0005
[WandB] Iter 58 | reward=0.00 | loss=0.0004
[WandB] Iter 59 | reward=0.00 | loss=0.0004
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage3.pt
=== Stage 4: Zielgeschwindigkeit 0.5 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-8.6958e-03,  8.0472e-03, -2.0741e-01, -6.8475e-04,  1.3328e-02,
         8.3822e-03, -4.3182e-04, -4.3866e-04, -5.0621e-04,  5.4248e-04,
        -5.1387e-04, -5.2249e-04, -5.3138e-04, -5.2916e-04, -2.8788e-03,
        -2.9244e-03, -3.3747e-03,  3.6165e-03, -3.4258e-03, -3.4833e-03,
        -3.5425e-03, -3.5277e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-0.7075, -1.1823, -0.4375,  0.4025, -1.1355, -2.5455, -0.7084, -1.7120],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0038, device='cuda:0')
[WandB] Iter 60 | reward=-0.00 | loss=0.0027
[WandB] Iter 61 | reward=-0.00 | loss=0.0010
[WandB] Iter 62 | reward=-0.00 | loss=0.0464
[WandB] Iter 63 | reward=-0.00 | loss=0.0007
[WandB] Iter 64 | reward=0.00 | loss=0.0008
[WandB] Iter 65 | reward=0.00 | loss=0.0006
[WandB] Iter 66 | reward=0.00 | loss=0.0006
[WandB] Iter 67 | reward=0.00 | loss=0.0009
[WandB] Iter 68 | reward=0.00 | loss=0.0006
[WandB] Iter 69 | reward=0.00 | loss=0.0006
[DEBUG] Observation (env 0): tensor([ 2.9476e-04, -2.1973e-02, -1.8989e-01,  4.2477e-04, -8.0506e-03,
        -2.2726e-02,  4.3291e-04,  4.4015e-04,  4.6364e-04,  4.5265e-04,
         4.9744e-04,  4.9038e-04,  5.3257e-04,  5.2661e-04,  2.8860e-03,
         2.9343e-03,  3.0910e-03,  3.0177e-03,  3.3163e-03,  3.2692e-03,
         3.5504e-03,  3.5108e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([0.9947, 0.0926, 0.3244, 0.8311, 0.7852, 0.3493, 0.3710, 0.3141],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0060, device='cuda:0')
[WandB] Iter 70 | reward=0.00 | loss=0.0006
[WandB] Iter 71 | reward=-0.00 | loss=0.0011
[WandB] Iter 72 | reward=-0.00 | loss=0.0007
[WandB] Iter 73 | reward=-0.00 | loss=0.0006
[WandB] Iter 74 | reward=0.00 | loss=0.0006
[WandB] Iter 75 | reward=0.00 | loss=0.0006
[WandB] Iter 76 | reward=0.00 | loss=0.0007
[WandB] Iter 77 | reward=0.00 | loss=0.0006
[WandB] Iter 78 | reward=0.00 | loss=0.0005
[WandB] Iter 79 | reward=0.00 | loss=0.0004
[DEBUG] Observation (env 0): tensor([ 8.7008e-03,  3.0915e-03, -1.8752e-01,  7.9916e-04, -9.5297e-03,
         3.0127e-03,  4.3535e-04,  4.3720e-04,  5.4714e-04, -4.9512e-04,
        -5.2347e-04, -5.1166e-04, -5.3075e-04,  5.3163e-04,  2.9023e-03,
         2.9147e-03,  3.6476e-03, -3.3008e-03, -3.4898e-03, -3.4110e-03,
        -3.5384e-03,  3.5442e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.0465,  1.1921,  0.5868, -0.8098, -0.1627, -0.5578, -1.8566,  0.9763],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0063, device='cuda:0')
[WandB] Iter 80 | reward=0.00 | loss=0.0008
[WandB] Iter 81 | reward=0.00 | loss=0.0010
[WandB] Iter 82 | reward=-0.00 | loss=0.0005
[WandB] Iter 83 | reward=0.00 | loss=0.0009
[WandB] Iter 84 | reward=0.00 | loss=0.0005
[WandB] Iter 85 | reward=0.00 | loss=0.0005
[WandB] Iter 86 | reward=0.00 | loss=0.0005
[WandB] Iter 87 | reward=0.00 | loss=0.0013
[WandB] Iter 88 | reward=0.00 | loss=0.0005
[WandB] Iter 89 | reward=0.00 | loss=0.0006
[WandB] Iter 90 | reward=0.00 | loss=0.0004
[DEBUG] Observation (env 0): tensor([-4.5232e-04,  1.1403e-02, -2.0367e-01, -9.7351e-03,  8.1243e-03,
         2.2215e-02,  5.1076e-04, -5.1691e-04, -4.6376e-04, -4.5405e-04,
        -5.0120e-04, -4.9258e-04, -5.3262e-04,  5.3306e-04,  3.4049e-03,
        -3.4461e-03, -3.0917e-03, -3.0270e-03, -3.3413e-03, -3.2838e-03,
        -3.5508e-03,  3.5537e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.9598, -0.4589, -1.8729,  0.1363, -0.5294, -1.0302, -0.6246,  1.1981],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(-0.0066, device='cuda:0')
[WandB] Iter 91 | reward=0.00 | loss=0.0004
[WandB] Iter 92 | reward=-0.00 | loss=0.0009
[WandB] Iter 93 | reward=0.00 | loss=0.0006
[WandB] Iter 94 | reward=0.00 | loss=0.0005
[WandB] Iter 95 | reward=0.00 | loss=0.0006
[WandB] Iter 96 | reward=0.00 | loss=0.0006
[WandB] Iter 97 | reward=0.00 | loss=0.0005
[WandB] Iter 98 | reward=0.00 | loss=0.0030
[WandB] Iter 99 | reward=0.00 | loss=0.0005
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_final.pt
=== Trained model saved at logs/dodo-walking/model_final.pt ===
