Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
=== Stage 1: 0.1 m/s ===
[WandB] Iter 0 | reward=0.00 | loss=0.1611
[WandB] Iter 0 | reward=0.00 | loss=0.1611
[Plot] saved to logs/test_logging/metrics.png
[WandB] Iter 1 | reward=-0.00 | loss=0.1229
[WandB] Iter 1 | reward=-0.00 | loss=0.1229
[WandB] Iter 2 | reward=0.00 | loss=0.1055
[WandB] Iter 2 | reward=0.00 | loss=0.1055
[WandB] Iter 3 | reward=0.01 | loss=0.0777
[WandB] Iter 3 | reward=0.01 | loss=0.0777
[WandB] Iter 4 | reward=0.01 | loss=0.0724
[WandB] Iter 4 | reward=0.01 | loss=0.0724
[WandB] Iter 5 | reward=0.01 | loss=0.0654
[WandB] Iter 5 | reward=0.01 | loss=0.0654
[WandB] Iter 6 | reward=0.02 | loss=0.0602
[WandB] Iter 6 | reward=0.02 | loss=0.0602
[WandB] Iter 7 | reward=0.02 | loss=0.0762
[WandB] Iter 7 | reward=0.02 | loss=0.0762
[WandB] Iter 8 | reward=0.02 | loss=0.0629
[WandB] Iter 8 | reward=0.02 | loss=0.0629
[WandB] Iter 9 | reward=0.02 | loss=0.0639
[WandB] Iter 9 | reward=0.02 | loss=0.0639
[WandB] Iter 10 | reward=0.03 | loss=0.0738
[WandB] Iter 10 | reward=0.03 | loss=0.0738
[WandB] Iter 11 | reward=0.03 | loss=0.0644
[WandB] Iter 11 | reward=0.03 | loss=0.0644
[WandB] Iter 12 | reward=0.03 | loss=0.0728
[WandB] Iter 12 | reward=0.03 | loss=0.0728
[WandB] Iter 13 | reward=0.03 | loss=0.0724
[WandB] Iter 13 | reward=0.03 | loss=0.0724
[WandB] Iter 14 | reward=0.04 | loss=0.0682
[WandB] Iter 14 | reward=0.04 | loss=0.0682
[WandB] Iter 15 | reward=0.04 | loss=0.0770
[WandB] Iter 15 | reward=0.04 | loss=0.0770
[WandB] Iter 16 | reward=0.04 | loss=0.0773
[WandB] Iter 16 | reward=0.04 | loss=0.0773
[WandB] Iter 17 | reward=0.04 | loss=0.0730
[WandB] Iter 17 | reward=0.04 | loss=0.0730
[WandB] Iter 18 | reward=0.04 | loss=0.0920
[WandB] Iter 18 | reward=0.04 | loss=0.0920
[WandB] Iter 19 | reward=0.05 | loss=0.0940
[WandB] Iter 19 | reward=0.05 | loss=0.0940
Traceback (most recent call last):
  File "/home/hoan/Desktop/Malte/MalteVSC/dodo_Genesis_RL/dodo_train.py", line 296, in <module>
    main()
  File "/home/hoan/Desktop/Malte/MalteVSC/dodo_Genesis_RL/dodo_train.py", line 291, in main
    runner.save(os.path.join(log_dir, fname))
  File "/home/hoan/miniconda3/envs/dodolab/lib/python3.12/site-packages/rsl_rl/runners/on_policy_runner.py", line 379, in save
    self.writer.save_model(path, self.current_learning_iteration)
    ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'save_model'
Traceback (most recent call last):
  File "/home/hoan/Desktop/Malte/MalteVSC/dodo_Genesis_RL/dodo_train.py", line 296, in <module>
    main()
  File "/home/hoan/Desktop/Malte/MalteVSC/dodo_Genesis_RL/dodo_train.py", line 291, in main
    runner.save(os.path.join(log_dir, fname))
  File "/home/hoan/miniconda3/envs/dodolab/lib/python3.12/site-packages/rsl_rl/runners/on_policy_runner.py", line 379, in save
    self.writer.save_model(path, self.current_learning_iteration)
    ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'save_model'

[38;5;9m[Genesis] [20:35:44] [ERROR] AttributeError: 'NoneType' object has no attribute 'save_model'[0m
