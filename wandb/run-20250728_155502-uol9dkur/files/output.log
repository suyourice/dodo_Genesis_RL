=== Stage 1: Zielgeschwindigkeit 0.1 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-0.0120, -0.0067, -0.2114, -0.0100,  0.0162,  0.0035,  0.0005, -0.0005,
        -0.0005,  0.0005, -0.0005,  0.0005, -0.0005,  0.0005,  0.0034, -0.0034,
        -0.0035,  0.0034, -0.0035,  0.0034, -0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.4274, -1.3232, -0.4172,  1.3371,  0.1626,  0.0850, -1.4033,  1.1682],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0278, device='cuda:0')
[WandB] Iter 0 | reward=0.02 | loss=0.0103
[WandB] Iter 0 | reward=0.02 | loss=0.0103
[Plot] saved to logs/dodo-walking/metrics.png
[WandB] Iter 1 | reward=0.02 | loss=0.0364
[WandB] Iter 1 | reward=0.02 | loss=0.0364
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage1.pt
=== Stage 2: Zielgeschwindigkeit 0.3 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-0.0054, -0.0011, -0.2031,  0.0006,  0.0054, -0.0011,  0.0004,  0.0004,
        -0.0005,  0.0005,  0.0005, -0.0005, -0.0005, -0.0005,  0.0029,  0.0029,
        -0.0035,  0.0035,  0.0035, -0.0035, -0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.4390,  0.9954, -3.2541,  0.6521,  2.0969, -1.8371, -1.2305, -0.3525],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0186, device='cuda:0')
[WandB] Iter 2 | reward=0.01 | loss=0.0030
[WandB] Iter 2 | reward=0.01 | loss=0.0030
[WandB] Iter 3 | reward=0.01 | loss=0.0104
[WandB] Iter 3 | reward=0.01 | loss=0.0104
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage2.pt
=== Stage 3: Zielgeschwindigkeit 0.4 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-0.0006,  0.0208, -0.2050,  0.0010,  0.0081,  0.0213,  0.0004,  0.0004,
        -0.0005, -0.0005, -0.0005, -0.0005,  0.0005,  0.0005,  0.0029,  0.0029,
        -0.0031, -0.0030, -0.0033, -0.0033,  0.0035,  0.0036,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.2650,  1.0363, -0.6743, -0.4197, -0.3647, -1.5811,  1.9881,  0.9056],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0145, device='cuda:0')
[WandB] Iter 4 | reward=0.01 | loss=0.0031
[WandB] Iter 4 | reward=0.01 | loss=0.0031
[WandB] Iter 5 | reward=0.01 | loss=0.0048
[WandB] Iter 5 | reward=0.01 | loss=0.0048
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage3.pt
=== Stage 4: Zielgeschwindigkeit 0.5 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-8.8727e-03, -1.3668e-02, -2.0605e-01, -1.0110e-02,  9.6039e-03,
        -3.5308e-03,  5.0836e-04, -5.1392e-04, -5.4727e-04,  4.9370e-04,
         5.1971e-04,  5.0946e-04,  5.3070e-04,  5.2804e-04,  3.3891e-03,
        -3.4261e-03, -3.6485e-03,  3.2913e-03,  3.4647e-03,  3.3964e-03,
         3.5380e-03,  3.5203e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.8400, -0.3139, -1.1091,  1.3275,  0.3079,  1.0252,  1.5902,  0.2506],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0139, device='cuda:0')
[WandB] Iter 6 | reward=0.01 | loss=0.0027
[WandB] Iter 6 | reward=0.01 | loss=0.0027
[WandB] Iter 7 | reward=0.01 | loss=0.0463
[WandB] Iter 7 | reward=0.01 | loss=0.0463
[WandB] Iter 8 | reward=0.01 | loss=0.0019
[WandB] Iter 8 | reward=0.01 | loss=0.0019
[WandB] Iter 9 | reward=0.01 | loss=0.0011
[WandB] Iter 9 | reward=0.01 | loss=0.0011
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_final.pt
=== Trained model saved at logs/dodo-walking/model_final.pt ===
