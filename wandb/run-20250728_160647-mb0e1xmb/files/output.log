=== Stage 1: Zielgeschwindigkeit 0.1 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-0.0005,  0.0003, -0.2012, -0.0098,  0.0043,  0.0108,  0.0005, -0.0005,
        -0.0005, -0.0005,  0.0005,  0.0005,  0.0005,  0.0005,  0.0034, -0.0034,
        -0.0034, -0.0033,  0.0036,  0.0036,  0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.5829, -0.3121, -1.4467, -1.0034,  0.8215, -0.0563,  1.9274,  1.6738],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0280, device='cuda:0')
[WandB] Iter 0 | reward=0.02 | loss=0.0081
[WandB] Iter 0 | reward=0.02 | loss=0.0081
[Plot] saved to logs/dodo-walking/metrics.png
[WandB] Iter 1 | reward=0.02 | loss=0.0106
[WandB] Iter 1 | reward=0.02 | loss=0.0106
[WandB] Iter 2 | reward=0.02 | loss=0.0249
[WandB] Iter 2 | reward=0.02 | loss=0.0249
[WandB] Iter 3 | reward=0.02 | loss=0.0111
[WandB] Iter 3 | reward=0.02 | loss=0.0111
[WandB] Iter 4 | reward=0.02 | loss=0.0162
[WandB] Iter 4 | reward=0.02 | loss=0.0162
[WandB] Iter 5 | reward=0.02 | loss=0.0129
[WandB] Iter 5 | reward=0.02 | loss=0.0129
[WandB] Iter 6 | reward=0.02 | loss=0.0044
[WandB] Iter 6 | reward=0.02 | loss=0.0044
[WandB] Iter 7 | reward=0.02 | loss=0.0019
[WandB] Iter 7 | reward=0.02 | loss=0.0019
[WandB] Iter 8 | reward=0.02 | loss=0.0013
[WandB] Iter 8 | reward=0.02 | loss=0.0013
[WandB] Iter 9 | reward=0.02 | loss=0.0009
[WandB] Iter 9 | reward=0.02 | loss=0.0009
[DEBUG] Observation (env 0): tensor([ 0.0087, -0.0169, -0.1842, -0.0101, -0.0127, -0.0069,  0.0005, -0.0005,
         0.0005, -0.0005,  0.0005,  0.0005,  0.0005,  0.0005,  0.0034, -0.0034,
         0.0034, -0.0036,  0.0034,  0.0035,  0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.5488, -1.1143,  0.2071, -0.7401,  0.4119,  1.4004,  0.5461,  0.4185],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0282, device='cuda:0')
[WandB] Iter 10 | reward=0.02 | loss=0.0013
[WandB] Iter 10 | reward=0.02 | loss=0.0013
[WandB] Iter 11 | reward=0.01 | loss=0.0061
[WandB] Iter 11 | reward=0.01 | loss=0.0061
[WandB] Iter 12 | reward=0.02 | loss=0.0009
[WandB] Iter 12 | reward=0.02 | loss=0.0009
[WandB] Iter 13 | reward=0.02 | loss=0.0010
[WandB] Iter 13 | reward=0.02 | loss=0.0010
[WandB] Iter 14 | reward=0.02 | loss=0.0010
[WandB] Iter 14 | reward=0.02 | loss=0.0010
[WandB] Iter 15 | reward=0.02 | loss=0.0007
[WandB] Iter 15 | reward=0.02 | loss=0.0007
[WandB] Iter 16 | reward=0.02 | loss=0.0003
[WandB] Iter 16 | reward=0.02 | loss=0.0003
[WandB] Iter 17 | reward=0.02 | loss=0.0003
[WandB] Iter 17 | reward=0.02 | loss=0.0003
[WandB] Iter 18 | reward=0.02 | loss=0.0002
[WandB] Iter 18 | reward=0.02 | loss=0.0002
[WandB] Iter 19 | reward=0.02 | loss=0.0001
[WandB] Iter 19 | reward=0.02 | loss=0.0001
[DEBUG] Observation (env 0): tensor([-0.0120, -0.0067, -0.2114, -0.0100,  0.0162,  0.0035,  0.0005, -0.0005,
        -0.0005,  0.0005, -0.0005,  0.0005, -0.0005,  0.0005,  0.0034, -0.0034,
        -0.0035,  0.0034, -0.0035,  0.0034, -0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.7494, -3.0492, -1.2319,  1.1027, -0.4371,  0.5645, -1.9229,  0.6900],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0278, device='cuda:0')
[WandB] Iter 20 | reward=0.02 | loss=0.0002
[WandB] Iter 20 | reward=0.02 | loss=0.0002
[WandB] Iter 21 | reward=0.01 | loss=0.0093
[WandB] Iter 21 | reward=0.01 | loss=0.0093
[WandB] Iter 22 | reward=0.02 | loss=0.0018
[WandB] Iter 22 | reward=0.02 | loss=0.0018
[WandB] Iter 23 | reward=0.02 | loss=0.0008
[WandB] Iter 23 | reward=0.02 | loss=0.0008
[WandB] Iter 24 | reward=0.02 | loss=0.0007
[WandB] Iter 24 | reward=0.02 | loss=0.0007
[WandB] Iter 25 | reward=0.02 | loss=0.0007
[WandB] Iter 25 | reward=0.02 | loss=0.0007
[WandB] Iter 26 | reward=0.02 | loss=0.0006
[WandB] Iter 26 | reward=0.02 | loss=0.0006
[WandB] Iter 27 | reward=0.02 | loss=0.0003
[WandB] Iter 27 | reward=0.02 | loss=0.0003
[WandB] Iter 28 | reward=0.02 | loss=0.0002
[WandB] Iter 28 | reward=0.02 | loss=0.0002
[WandB] Iter 29 | reward=0.02 | loss=0.0001
[WandB] Iter 29 | reward=0.02 | loss=0.0001
[WandB] Iter 30 | reward=0.02 | loss=0.0001
[WandB] Iter 30 | reward=0.02 | loss=0.0001
[DEBUG] Observation (env 0): tensor([-0.0036,  0.0079, -0.2067, -0.0098,  0.0113,  0.0186,  0.0005, -0.0005,
        -0.0005, -0.0005, -0.0005,  0.0006,  0.0005, -0.0005,  0.0034, -0.0034,
        -0.0032, -0.0032, -0.0034,  0.0037,  0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.7721, -0.1677, -1.2742, -0.9196, -0.8265,  0.5876,  0.9621, -2.4411],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0279, device='cuda:0')
[WandB] Iter 31 | reward=0.02 | loss=0.0028
[WandB] Iter 31 | reward=0.02 | loss=0.0028
[WandB] Iter 32 | reward=0.02 | loss=0.0066
[WandB] Iter 32 | reward=0.02 | loss=0.0066
[WandB] Iter 33 | reward=0.02 | loss=0.0007
[WandB] Iter 33 | reward=0.02 | loss=0.0007
[WandB] Iter 34 | reward=0.02 | loss=0.0005
[WandB] Iter 34 | reward=0.02 | loss=0.0005
[WandB] Iter 35 | reward=0.02 | loss=0.0004
[WandB] Iter 35 | reward=0.02 | loss=0.0004
[WandB] Iter 36 | reward=0.02 | loss=0.0004
[WandB] Iter 36 | reward=0.02 | loss=0.0004
[WandB] Iter 37 | reward=0.02 | loss=0.0003
[WandB] Iter 37 | reward=0.02 | loss=0.0003
[WandB] Iter 38 | reward=0.02 | loss=0.0003
[WandB] Iter 38 | reward=0.02 | loss=0.0003
[WandB] Iter 39 | reward=0.02 | loss=0.0002
[WandB] Iter 39 | reward=0.02 | loss=0.0002
[WandB] Iter 40 | reward=0.02 | loss=0.0002
[WandB] Iter 40 | reward=0.02 | loss=0.0002
[DEBUG] Observation (env 0): tensor([ 0.0087,  0.0047, -0.1859, -0.0006, -0.0088,  0.0049, -0.0004, -0.0004,
         0.0005, -0.0005, -0.0005, -0.0005, -0.0005,  0.0005, -0.0029, -0.0029,
         0.0036, -0.0033, -0.0035, -0.0034, -0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-0.9294, -1.0034,  1.4559,  0.0585, -0.6122, -1.8804, -0.2462,  0.2194],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0282, device='cuda:0')
[WandB] Iter 41 | reward=0.02 | loss=0.0003
[WandB] Iter 41 | reward=0.02 | loss=0.0003
[WandB] Iter 42 | reward=0.01 | loss=0.0122
[WandB] Iter 42 | reward=0.01 | loss=0.0122
[WandB] Iter 43 | reward=0.02 | loss=0.0009
[WandB] Iter 43 | reward=0.02 | loss=0.0009
