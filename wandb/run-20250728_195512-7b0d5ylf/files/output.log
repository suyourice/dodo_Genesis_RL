=== Stage 1: Zielgeschwindigkeit 0.1 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([ 0.0003, -0.0220, -0.1899,  0.0004, -0.0081, -0.0227,  0.0004,  0.0004,
         0.0005,  0.0005,  0.0005,  0.0005,  0.0005,  0.0005,  0.0029,  0.0029,
         0.0031,  0.0030,  0.0033,  0.0033,  0.0036,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([1.1296, 0.8449, 0.3921, 0.8752, 0.5566, 1.5339, 0.5256, 1.3313],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0230, device='cuda:0')
[WandB] Iter 0 | reward=0.02 | loss=0.0128
[Plot] saved to logs/dodo-walking/metrics.png
[WandB] Iter 1 | reward=0.01 | loss=0.0082
[WandB] Iter 2 | reward=0.02 | loss=0.0340
[WandB] Iter 3 | reward=0.02 | loss=0.0081
[WandB] Iter 4 | reward=0.02 | loss=0.0105
[WandB] Iter 5 | reward=0.02 | loss=0.0091
[WandB] Iter 6 | reward=0.02 | loss=0.0039
[WandB] Iter 7 | reward=0.02 | loss=0.0026
[WandB] Iter 8 | reward=0.03 | loss=0.0018
[WandB] Iter 9 | reward=0.03 | loss=0.0012
[DEBUG] Observation (env 0): tensor([ 0.0117, -0.0127, -0.1814, -0.0101, -0.0156, -0.0026,  0.0005, -0.0005,
         0.0005, -0.0005,  0.0005, -0.0005,  0.0005,  0.0005,  0.0034, -0.0034,
         0.0035, -0.0034,  0.0035, -0.0035,  0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.0102, -0.0641,  0.1662, -0.4096,  0.6268, -0.8933,  2.7514,  0.4006],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0235, device='cuda:0')
[WandB] Iter 10 | reward=0.02 | loss=0.0012
[WandB] Iter 11 | reward=0.01 | loss=0.0109
[WandB] Iter 12 | reward=0.02 | loss=0.0021
[WandB] Iter 13 | reward=0.02 | loss=0.0008
[WandB] Iter 14 | reward=0.02 | loss=0.0010
[WandB] Iter 15 | reward=0.02 | loss=0.0013
[WandB] Iter 16 | reward=0.02 | loss=0.0012
[WandB] Iter 17 | reward=0.02 | loss=0.0011
[WandB] Iter 18 | reward=0.02 | loss=0.0011
[WandB] Iter 19 | reward=0.02 | loss=0.0009
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage1.pt
=== Stage 2: Zielgeschwindigkeit 0.3 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-0.0035,  0.0072, -0.2065, -0.0097,  0.0110,  0.0179,  0.0005, -0.0005,
        -0.0005, -0.0005, -0.0005,  0.0005, -0.0005,  0.0005,  0.0034, -0.0034,
        -0.0032, -0.0032, -0.0034,  0.0037, -0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.3273, -0.2721, -0.5621, -0.7650, -0.3632,  0.9936, -0.2759,  0.4035],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0187, device='cuda:0')
[WandB] Iter 20 | reward=0.01 | loss=0.0045
[WandB] Iter 21 | reward=0.01 | loss=0.0065
[WandB] Iter 22 | reward=0.02 | loss=0.0079
[WandB] Iter 23 | reward=0.02 | loss=0.0055
[WandB] Iter 24 | reward=0.02 | loss=0.0133
[WandB] Iter 25 | reward=0.02 | loss=0.0054
[WandB] Iter 26 | reward=0.02 | loss=0.0035
[WandB] Iter 27 | reward=0.02 | loss=0.0019
[WandB] Iter 28 | reward=0.02 | loss=0.0012
[WandB] Iter 29 | reward=0.02 | loss=0.0010
[DEBUG] Observation (env 0): tensor([ 0.0004, -0.0195, -0.1913, -0.0103, -0.0039, -0.0097,  0.0005, -0.0005,
         0.0005,  0.0005, -0.0005, -0.0005, -0.0005, -0.0005,  0.0034, -0.0034,
         0.0034,  0.0033, -0.0036, -0.0036, -0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.2599, -0.1889,  0.1092,  1.9619, -0.2591, -1.1755, -0.9170, -2.5793],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0189, device='cuda:0')
[WandB] Iter 30 | reward=0.02 | loss=0.0010
[WandB] Iter 31 | reward=0.01 | loss=0.0166
[WandB] Iter 32 | reward=0.01 | loss=0.0021
[WandB] Iter 33 | reward=0.02 | loss=0.0012
[WandB] Iter 34 | reward=0.02 | loss=0.0011
[WandB] Iter 35 | reward=0.02 | loss=0.0011
[WandB] Iter 36 | reward=0.02 | loss=0.0010
[WandB] Iter 37 | reward=0.02 | loss=0.0010
[WandB] Iter 38 | reward=0.02 | loss=0.0009
[WandB] Iter 39 | reward=0.02 | loss=0.0007
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage2.pt
=== Stage 3: Zielgeschwindigkeit 0.4 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([ 0.0056, -0.0004, -0.1906,  0.0008, -0.0064, -0.0006,  0.0004,  0.0004,
         0.0005, -0.0005, -0.0005,  0.0005,  0.0005, -0.0005,  0.0029,  0.0029,
         0.0035, -0.0035, -0.0035,  0.0035,  0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 1.6050,  0.9336,  0.8823,  0.0407, -1.1303,  1.4309,  0.8492, -0.1992],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0169, device='cuda:0')
[WandB] Iter 40 | reward=0.01 | loss=0.0046
[WandB] Iter 41 | reward=0.01 | loss=0.0059
[WandB] Iter 42 | reward=0.01 | loss=0.0061
[WandB] Iter 43 | reward=0.02 | loss=0.0053
[WandB] Iter 44 | reward=0.02 | loss=0.0030
[WandB] Iter 45 | reward=0.02 | loss=0.0022
[WandB] Iter 46 | reward=0.02 | loss=0.0015
[WandB] Iter 47 | reward=0.02 | loss=0.0011
[WandB] Iter 48 | reward=0.02 | loss=0.0009
[WandB] Iter 49 | reward=0.02 | loss=0.0007
[DEBUG] Observation (env 0): tensor([ 0.0120, -0.0037, -0.1818,  0.0006, -0.0164, -0.0039,  0.0004,  0.0004,
         0.0005, -0.0005,  0.0005, -0.0005,  0.0005, -0.0005,  0.0029,  0.0029,
         0.0035, -0.0034,  0.0035, -0.0034,  0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.8338,  0.4716,  0.3358, -0.5390,  2.6742, -1.4195,  1.1310, -1.4231],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0170, device='cuda:0')
[WandB] Iter 50 | reward=0.02 | loss=0.0008
[WandB] Iter 51 | reward=0.01 | loss=0.0143
[WandB] Iter 52 | reward=0.01 | loss=0.0026
[WandB] Iter 53 | reward=0.02 | loss=0.0011
[WandB] Iter 54 | reward=0.02 | loss=0.0011
[WandB] Iter 55 | reward=0.02 | loss=0.0011
[WandB] Iter 56 | reward=0.02 | loss=0.0013
[WandB] Iter 57 | reward=0.02 | loss=0.0011
[WandB] Iter 58 | reward=0.02 | loss=0.0010
[WandB] Iter 59 | reward=0.02 | loss=0.0009
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_stage3.pt
=== Stage 4: Zielgeschwindigkeit 0.5 m/s ===
Actor MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=8, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=33, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
[DEBUG] Observation (env 0): tensor([-3.5329e-03,  1.7651e-02, -2.0568e-01, -3.2721e-04,  1.1133e-02,
         1.8263e-02, -4.3353e-04, -4.3948e-04, -4.8483e-04, -4.8728e-04,
        -5.0573e-04,  5.4897e-04, -5.3195e-04,  5.3143e-04, -2.8903e-03,
        -2.9299e-03, -3.2322e-03, -3.2485e-03, -3.3715e-03,  3.6598e-03,
        -3.5464e-03,  3.5429e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-0.2530, -0.0721, -0.6436, -0.5166, -1.1336,  1.8153, -1.1215,  0.1326],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0152, device='cuda:0')
[WandB] Iter 60 | reward=0.01 | loss=0.0046
[WandB] Iter 61 | reward=0.01 | loss=0.0029
[WandB] Iter 62 | reward=0.01 | loss=0.0800
[WandB] Iter 63 | reward=0.01 | loss=0.0061
[WandB] Iter 64 | reward=0.02 | loss=0.0050
[WandB] Iter 65 | reward=0.02 | loss=0.0025
[WandB] Iter 66 | reward=0.02 | loss=0.0022
[WandB] Iter 67 | reward=0.02 | loss=0.0013
[WandB] Iter 68 | reward=0.02 | loss=0.0011
[WandB] Iter 69 | reward=0.02 | loss=0.0011
[DEBUG] Observation (env 0): tensor([ 0.0120,  0.0060, -0.1808,  0.0100, -0.0165, -0.0043, -0.0005,  0.0005,
         0.0005, -0.0005,  0.0005, -0.0005, -0.0005,  0.0005, -0.0034,  0.0034,
         0.0035, -0.0034,  0.0035, -0.0035, -0.0035,  0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-0.4211,  0.8839,  0.5940, -1.3913,  1.3478, -1.4631, -0.8321,  0.4798],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0155, device='cuda:0')
[WandB] Iter 70 | reward=0.02 | loss=0.0008
[WandB] Iter 71 | reward=0.01 | loss=0.0156
[WandB] Iter 72 | reward=0.01 | loss=0.0009
[WandB] Iter 73 | reward=0.01 | loss=0.0011
[WandB] Iter 74 | reward=0.02 | loss=0.0012
[WandB] Iter 75 | reward=0.02 | loss=0.0013
[WandB] Iter 76 | reward=0.02 | loss=0.0010
[WandB] Iter 77 | reward=0.02 | loss=0.0009
[WandB] Iter 78 | reward=0.02 | loss=0.0008
[WandB] Iter 79 | reward=0.02 | loss=0.0007
[DEBUG] Observation (env 0): tensor([ 0.0088, -0.0167, -0.1839, -0.0102, -0.0130, -0.0067,  0.0005, -0.0005,
         0.0005, -0.0005,  0.0005,  0.0005,  0.0005, -0.0005,  0.0034, -0.0034,
         0.0034, -0.0036,  0.0034,  0.0035,  0.0035, -0.0035,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,
         0.0000], device='cuda:0')
[DEBUG] Action      (env 0): tensor([ 0.1843, -1.7322,  0.3511,  0.0760,  0.4541,  0.9284,  0.2156, -0.3476],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0155, device='cuda:0')
[WandB] Iter 80 | reward=0.02 | loss=0.0006
[WandB] Iter 81 | reward=0.01 | loss=0.0034
[WandB] Iter 82 | reward=0.01 | loss=0.0029
[WandB] Iter 83 | reward=0.01 | loss=0.0009
[WandB] Iter 84 | reward=0.02 | loss=0.0010
[WandB] Iter 85 | reward=0.02 | loss=0.0006
[WandB] Iter 86 | reward=0.02 | loss=0.0006
[WandB] Iter 87 | reward=0.02 | loss=0.0005
[WandB] Iter 88 | reward=0.02 | loss=0.0005
[WandB] Iter 89 | reward=0.02 | loss=0.0005
[WandB] Iter 90 | reward=0.02 | loss=0.0005
[DEBUG] Observation (env 0): tensor([ 2.4321e-04,  2.9070e-04, -1.9179e-01,  9.8560e-03, -3.7900e-03,
        -1.0248e-02, -5.0735e-04,  5.1535e-04,  5.0665e-04,  5.0260e-04,
        -5.3459e-04, -5.4100e-04,  5.3051e-04, -5.3054e-04, -3.3824e-03,
         3.4357e-03,  3.3777e-03,  3.3507e-03, -3.5640e-03, -3.6067e-03,
         3.5367e-03, -3.5369e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         5.0000e-01,  0.0000e+00,  0.0000e+00], device='cuda:0')
[DEBUG] Action      (env 0): tensor([-0.2258,  1.2249,  2.3630,  2.3195, -1.5355, -2.0850,  0.9961, -1.9229],
       device='cuda:0')
[DEBUG] Reward      (env 0): tensor(0.0154, device='cuda:0')
[WandB] Iter 91 | reward=0.02 | loss=0.0014
[WandB] Iter 92 | reward=0.01 | loss=0.0035
[WandB] Iter 93 | reward=0.01 | loss=0.0016
[WandB] Iter 94 | reward=0.01 | loss=0.0010
[WandB] Iter 95 | reward=0.01 | loss=0.0009
[WandB] Iter 96 | reward=0.02 | loss=0.0007
[WandB] Iter 97 | reward=0.02 | loss=0.0008
[WandB] Iter 98 | reward=0.02 | loss=0.0008
[WandB] Iter 99 | reward=0.02 | loss=0.0017
[CustomRunner] ✅ Saved checkpoint to logs/dodo-walking/model_final.pt
=== Trained model saved at logs/dodo-walking/model_final.pt ===
